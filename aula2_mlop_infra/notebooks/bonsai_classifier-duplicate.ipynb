{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd3bb5bb",
   "metadata": {},
   "source": [
    "## MLflow 3.3.1 Best Practices and MLOps Workflow\n",
    "\n",
    "This notebook demonstrates a complete MLOps workflow using MLflow 3.3.1, including:\n",
    "\n",
    "- **Experiment Tracking:** Logging parameters, metrics, and artifacts (confusion matrix, predictions CSV) for reproducibility.\n",
    "- **Model Registry:** Registering models, managing lifecycle stages (Staging, Production), and promoting models when approved.\n",
    "- **Artifact Logging:** Ensuring all relevant outputs are saved for future analysis and auditability.\n",
    "- **Automation:** GitHub Actions for notebook cleaning and dependency updates to maintain code quality and security.\n",
    "\n",
    "The workflow is designed for educational clarity, following best practices for experiment management, model versioning, and CI/CD automation in MLOps projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b0a242-4f18-4bf0-8013-fa04e9337838",
   "metadata": {},
   "source": [
    "# Class II: Infrastructure as Code for MLOps\n",
    "\n",
    "## üå± Project: Bonsai Species Classifier for Plant E-commerce\n",
    "\n",
    "Welcome to our hands-on MLOps session! We're building a **bonsai species classifier** for a plant website that will:\n",
    "- **Identify bonsai species** from plant measurements\n",
    "- **Provide care recommendations** based on species type\n",
    "- **Help customers** choose the right bonsai for their needs\n",
    "\n",
    "### Infrastructure Stack (All Containerized!)\n",
    "- **MLflow**: For experiment tracking and model registry\n",
    "- **Docker**: All services are containerized (no local installation needed!)\n",
    "- **JupyterLab**: You're running this from a container right now\n",
    "- **API**: Model serving for the plant website\n",
    "\n",
    "## Quick Setup Check\n",
    "Make sure all containers are running:\n",
    "- MLflow UI: http://localhost:5001 (track bonsai model experiments)\n",
    "- JupyterLab: http://localhost:8888 (you're here!)\n",
    "- API: http://localhost:8080 (bonsai species prediction service)\n",
    "\n",
    "Let's start building our bonsai classifier! üå≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "752f67cc-54cf-40f2-afe7-387993031cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing MLflow...\n",
      "Collecting mlflow==3.3.1\n",
      "  Downloading mlflow-3.3.1-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting mlflow-skinny==3.3.1 (from mlflow==3.3.1)\n",
      "  Downloading mlflow_skinny-3.3.1-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting mlflow-tracing==3.3.1 (from mlflow==3.3.1)\n",
      "  Downloading mlflow_tracing-3.3.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting Flask<4 (from mlflow==3.3.1)\n",
      "  Downloading flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /opt/conda/lib/python3.11/site-packages (from mlflow==3.3.1) (1.12.0)\n",
      "Collecting cryptography<46,>=43.0.0 (from mlflow==3.3.1)\n",
      "  Downloading cryptography-45.0.7-cp311-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting docker<8,>=4.0.0 (from mlflow==3.3.1)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting graphene<4 (from mlflow==3.3.1)\n",
      "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting gunicorn<24 (from mlflow==3.3.1)\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting matplotlib<4 (from mlflow==3.3.1)\n",
      "  Downloading matplotlib-3.10.6-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting numpy<3 (from mlflow==3.3.1)\n",
      "  Downloading numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas<3 (from mlflow==3.3.1)\n",
      "  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow<22,>=4.0.0 (from mlflow==3.3.1)\n",
      "  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting scikit-learn<2 (from mlflow==3.3.1)\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy<2 (from mlflow==3.3.1)\n",
      "  Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from mlflow==3.3.1) (2.0.22)\n",
      "Collecting cachetools<7,>=5.0.0 (from mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading cachetools-6.2.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting click<9,>=7.0 (from mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting cloudpickle<4 (from mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading databricks_sdk-0.65.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting fastapi<1 (from mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting gitpython<4,>=3.1.9 (from mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==3.3.1->mlflow==3.3.1) (6.8.0)\n",
      "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: packaging<26 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==3.3.1->mlflow==3.3.1) (23.2)\n",
      "Collecting protobuf<7,>=3.12.0 (from mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting pydantic<3,>=1.10.8 (from mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml<7,>=5.1 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==3.3.1->mlflow==3.3.1) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==3.3.1->mlflow==3.3.1) (2.31.0)\n",
      "Collecting sqlparse<1,>=0.4.0 (from mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading sqlparse-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==3.3.1->mlflow==3.3.1) (4.8.0)\n",
      "Collecting uvicorn<1 (from mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.11/site-packages (from alembic!=1.10.0,<2->mlflow==3.3.1) (1.2.4)\n",
      "Requirement already satisfied: cffi>=1.14 in /opt/conda/lib/python3.11/site-packages (from cryptography<46,>=43.0.0->mlflow==3.3.1) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from docker<8,>=4.0.0->mlflow==3.3.1) (2.0.7)\n",
      "Collecting blinker>=1.9.0 (from Flask<4->mlflow==3.3.1)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting itsdangerous>=2.2.0 (from Flask<4->mlflow==3.3.1)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in /opt/conda/lib/python3.11/site-packages (from Flask<4->mlflow==3.3.1) (3.1.2)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from Flask<4->mlflow==3.3.1) (2.1.3)\n",
      "Collecting werkzeug>=3.1.0 (from Flask<4->mlflow==3.3.1)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow==3.3.1)\n",
      "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow==3.3.1)\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /opt/conda/lib/python3.11/site-packages (from graphene<4->mlflow==3.3.1) (2.8.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib<4->mlflow==3.3.1)\n",
      "  Downloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib<4->mlflow==3.3.1)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib<4->mlflow==3.3.1)\n",
      "  Downloading fonttools-4.59.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (109 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m109.7/109.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib<4->mlflow==3.3.1)\n",
      "  Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting pillow>=8 (from matplotlib<4->mlflow==3.3.1)\n",
      "  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib<4->mlflow==3.3.1)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas<3->mlflow==3.3.1) (2023.3.post1)\n",
      "Collecting tzdata>=2022.7 (from pandas<3->mlflow==3.3.1)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn<2->mlflow==3.3.1)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn<2->mlflow==3.3.1)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy<3,>=1.4.0->mlflow==3.3.1) (3.0.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.14->cryptography<46,>=43.0.0->mlflow==3.3.1) (2.21)\n",
      "Collecting google-auth~=2.0 (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting starlette<0.48.0,>=0.40.0 (from fastapi<1->mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading starlette-0.47.3-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.3.1->mlflow==3.3.1) (3.17.0)\n",
      "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.10.8->mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.10.8->mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-extensions<5,>=4.0.0 (from mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.10.8->mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow==3.3.1) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.3.1->mlflow==3.3.1) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.3.1->mlflow==3.3.1) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.3.1->mlflow==3.3.1) (2023.7.22)\n",
      "Collecting h11>=0.8 (from uvicorn<1->mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting cachetools<7,>=5.0.0 (from mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /opt/conda/lib/python3.11/site-packages (from starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.3.1->mlflow==3.3.1) (4.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.3.1->mlflow==3.3.1) (1.3.0)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.3.1->mlflow==3.3.1)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading mlflow-3.3.1-py3-none-any.whl (26.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m702.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading mlflow_skinny-3.3.1-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m780.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading mlflow_tracing-3.3.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m643.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cryptography-45.0.7-cp311-abi3-manylinux_2_34_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m840.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading flask-3.1.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.6-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m938.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m35.4/35.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m425.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (355 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m355.2/355.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading databricks_sdk-0.65.0-py3-none-any.whl (705 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m705.9/705.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.116.1-py3-none-any.whl (95 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m95.6/95.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fonttools-4.59.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m208.2/208.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m308.4/308.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m322.0/322.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sqlparse-0.5.3-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m216.1/216.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading starlette-0.47.3-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m73.0/73.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m83.1/83.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: werkzeug, tzdata, typing-extensions, threadpoolctl, sqlparse, smmap, pyparsing, pyasn1, pyarrow, protobuf, pillow, numpy, kiwisolver, joblib, itsdangerous, h11, gunicorn, graphql-core, fonttools, cycler, cloudpickle, click, cachetools, blinker, annotated-types, uvicorn, typing-inspection, starlette, scipy, rsa, pydantic-core, pyasn1-modules, pandas, opentelemetry-api, graphql-relay, gitdb, Flask, docker, cryptography, contourpy, scikit-learn, pydantic, opentelemetry-semantic-conventions, matplotlib, graphene, google-auth, gitpython, opentelemetry-sdk, fastapi, databricks-sdk, mlflow-tracing, mlflow-skinny, mlflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "  Attempting uninstall: blinker\n",
      "    Found existing installation: blinker 1.6.3\n",
      "    Uninstalling blinker-1.6.3:\n",
      "      Successfully uninstalled blinker-1.6.3\n",
      "  Attempting uninstall: cryptography\n",
      "    Found existing installation: cryptography 41.0.4\n",
      "    Uninstalling cryptography-41.0.4:\n",
      "      Successfully uninstalled cryptography-41.0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyopenssl 23.2.0 requires cryptography!=40.0.0,!=40.0.1,<42,>=38.0.0, but you have cryptography 45.0.7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed Flask-3.1.2 annotated-types-0.7.0 blinker-1.9.0 cachetools-5.5.2 click-8.2.1 cloudpickle-3.1.1 contourpy-1.3.3 cryptography-45.0.7 cycler-0.12.1 databricks-sdk-0.65.0 docker-7.1.0 fastapi-0.116.1 fonttools-4.59.2 gitdb-4.0.12 gitpython-3.1.45 google-auth-2.40.3 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 h11-0.16.0 itsdangerous-2.2.0 joblib-1.5.2 kiwisolver-1.4.9 matplotlib-3.10.6 mlflow-3.3.1 mlflow-skinny-3.3.1 mlflow-tracing-3.3.1 numpy-2.3.2 opentelemetry-api-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 pandas-2.3.2 pillow-11.3.0 protobuf-6.32.0 pyarrow-21.0.0 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.11.7 pydantic-core-2.33.2 pyparsing-3.2.3 rsa-4.9.1 scikit-learn-1.7.1 scipy-1.16.1 smmap-5.0.2 sqlparse-0.5.3 starlette-0.47.3 threadpoolctl-3.6.0 typing-extensions-4.15.0 typing-inspection-0.4.1 tzdata-2025.2 uvicorn-0.35.0 werkzeug-3.1.3\n",
      "üéØ MLflow Tracking URI: http://mlflow:5000\n",
      "üöÄ Ready to track experiments!\n"
     ]
    }
   ],
   "source": [
    "# First, let's install MLflow in this container and set up the connection\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install MLflow if not already installed\n",
    "try:\n",
    "    import mlflow\n",
    "    print(\"‚úÖ MLflow already installed\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing MLflow...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"mlflow==3.3.1\"])\n",
    "    import mlflow\n",
    "\n",
    "# Set the MLflow tracking URI to connect to our containerized MLflow server\n",
    "import os\n",
    "os.environ['MLFLOW_TRACKING_URI'] = 'http://mlflow:5000'\n",
    "mlflow.set_tracking_uri('http://mlflow:5000')\n",
    "\n",
    "print(f\"üéØ MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(\"üöÄ Ready to track experiments!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab780523-a468-4879-8f2d-c50abd911569",
   "metadata": {},
   "source": [
    "# üå≥ Bonsai Species Classification with MLflow\n",
    "\n",
    "Now let's train our bonsai species classifier and track the experiment. We'll classify 4 types of bonsai:\n",
    "- **Juniper Bonsai** (0): Hardy, needle-like foliage\n",
    "- **Ficus Bonsai** (1): Broad leaves, aerial roots  \n",
    "- **Pine Bonsai** (2): Long needles, rugged bark\n",
    "- **Maple Bonsai** (3): Distinctive lobed leaves\n",
    "\n",
    "We'll track:\n",
    "- **Parameters**: Model configuration (n_estimators, etc.)\n",
    "- **Metrics**: Classification performance (accuracy, etc.)\n",
    "- **Artifacts**: The trained bonsai classifier model\n",
    "\n",
    "Check the MLflow UI at http://localhost:5001 to see your bonsai classification experiments! üå±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a96a8720-eca0-4173-9ed5-a5976750862c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå± Bonsai Dataset created:\n",
      "Training samples: 240\n",
      "Test samples: 60\n",
      "Features: ['leaf_length_cm', 'leaf_width_cm', 'branch_thickness_mm', 'height_cm']\n",
      "Species classes: ['Juniper', 'Ficus', 'Pine', 'Maple']\n",
      "\n",
      "üìä Sample bonsai measurements:\n",
      "   leaf_length_cm  leaf_width_cm  branch_thickness_mm  height_cm species\n",
      "0        1.380904       1.188396             1.650313  19.065886   Ficus\n",
      "1        1.893503       1.425476             5.341917  30.000305   Maple\n",
      "2        2.007036       1.334003             7.963199  24.248633   Maple\n",
      "3        2.892563       1.966694             5.784386  14.109763   Maple\n",
      "4        3.342822       2.035872             5.564911   9.854393   Maple\n"
     ]
    }
   ],
   "source": [
    "import mlflow.sklearn\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a simulated bonsai dataset (replacing Iris for our bonsai classifier)\n",
    "# Features: leaf_length, leaf_width, branch_thickness, height\n",
    "X, y = make_classification(\n",
    "    n_samples=300,\n",
    "    n_features=4,\n",
    "    n_classes=4,\n",
    "    n_informative=4,\n",
    "    n_redundant=0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Add realistic feature names and scaling for bonsai measurements\n",
    "feature_names = ['leaf_length_cm', 'leaf_width_cm', 'branch_thickness_mm', 'height_cm']\n",
    "bonsai_species = ['Juniper', 'Ficus', 'Pine', 'Maple']\n",
    "\n",
    "# Scale features to realistic bonsai measurements\n",
    "X[:, 0] = X[:, 0] * 0.5 + 2.0  # leaf_length: 1.5-2.5 cm\n",
    "X[:, 1] = X[:, 1] * 0.3 + 1.5  # leaf_width: 1.2-1.8 cm\n",
    "X[:, 2] = X[:, 2] * 2.0 + 5.0  # branch_thickness: 3-7 mm\n",
    "X[:, 3] = X[:, 3] * 10.0 + 25.0  # height: 15-35 cm\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "bonsai_df = pd.DataFrame(X, columns=feature_names)\n",
    "bonsai_df['species'] = [bonsai_species[i] for i in y]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"üå± Bonsai Dataset created:\")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "print(f\"Features: {feature_names}\")\n",
    "print(f\"Species classes: {bonsai_species}\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nüìä Sample bonsai measurements:\")\n",
    "print(bonsai_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dc2b01",
   "metadata": {},
   "source": [
    "# üÜï MLflow 3.3.1 Dataset Feature\n",
    "MLflow Datasets allow you to track, version, and reuse input data for your experiments. This ensures reproducibility and makes it easy to compare results across different runs.\n",
    "- **Track the exact data used for each run**\n",
    "- **Version datasets for auditability**\n",
    "- **Share and reuse datasets in future experiments**\n",
    "Let's log our bonsai dataset using MLflow's new Dataset API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067309aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Log the bonsai dataset using MLflow Datasets (v3.3.1+)\n",
    "import mlflow.data\n",
    "\n",
    "bonsai_df.to_csv(f\"bonsai_dataset_{date}.csv\", index=False)\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    dataset = mlflow.data.from_pandas(\n",
    "        bonsai_df,\n",
    "        source=f\"bonsai_dataset_{date}.csv\",\n",
    "        name=\"bonsai_species_measurements\"\n",
    "    )\n",
    "    mlflow.log_input(dataset, context=\"training\")\n",
    "    print(\"‚úÖ Bonsai dataset logged as MLflow Dataset!\")\n",
    "    run_id = run.info.run_id\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6cda93-8518-46f1-8c8c-53800636a243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments with different configurations to compare performance\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import mlflow.data\n",
    "from datetime import datetime\n",
    "\n",
    "date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# First thing version our dataset\n",
    "bonsai_df.to_csv(f\"bonsai_dataset_{date}.csv\", index=False)\n",
    "\n",
    "dataset = mlflow.data.from_pandas(\n",
    "    bonsai_df,\n",
    "    source=f\"bonsai_dataset_{date}.csv\",\n",
    "    name=f\"bonsai_species_measurements_{date}\",\n",
    "    targets=\"species\" if \"species\" in bonsai_df.columns else None,\n",
    ")\n",
    "# Set up experiment for bonsai classification\n",
    "mlflow.set_experiment(\"Bonsai-Species-Classification\")\n",
    "\n",
    "# List of configurations to test\n",
    "experiment_configs = [\n",
    "    {\n",
    "        \"name\": \"baseline_model\",\n",
    "        \"n_estimators\": 50,\n",
    "        \"max_depth\": 3,\n",
    "        \"min_samples_split\": 2,\n",
    "        \"description\": \"Baseline model with conservative settings\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"balanced_model\", \n",
    "        \"n_estimators\": 100,\n",
    "        \"max_depth\": 5,\n",
    "        \"min_samples_split\": 3,\n",
    "        \"description\": \"Balanced model between complexity and performance\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"complex_model\",\n",
    "        \"n_estimators\": 200,\n",
    "        \"max_depth\": 8,\n",
    "        \"min_samples_split\": 2,\n",
    "        \"description\": \"More complex model for maximum performance\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"optimized_model\",\n",
    "        \"n_estimators\": 150,\n",
    "        \"max_depth\": 6,\n",
    "        \"min_samples_split\": 4,\n",
    "        \"description\": \"Optimized model based on previous results\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üß™ Running multiple experiments for comparison...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run experiments\n",
    "experiment_results = []\n",
    "\n",
    "for config in experiment_configs:\n",
    "    with mlflow.start_run(run_name=config[\"name\"]):\n",
    "\n",
    "        # Log the bonsai dataset using MLflow Datasets\n",
    "        mlflow.log_input(dataset, context=\"training\")\n",
    "\n",
    "        \n",
    "        # Train model with specific configuration\n",
    "        bonsai_classifier = RandomForestClassifier(\n",
    "            n_estimators=config[\"n_estimators\"],\n",
    "            max_depth=config[\"max_depth\"],\n",
    "            min_samples_split=config[\"min_samples_split\"],\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "\n",
    "        bonsai_classifier.fit(X_train, y_train)\n",
    "        preds = bonsai_classifier.predict(X_test)\n",
    "        print(\"‚úÖ Bonsai dataset logged as MLflow Dataset!\")\n",
    "        \n",
    "        # Calculate detailed metrics\n",
    "        accuracy = accuracy_score(y_test, preds)\n",
    "        precision = precision_score(y_test, preds, average='weighted')\n",
    "        recall = recall_score(y_test, preds, average='weighted')\n",
    "        f1 = f1_score(y_test, preds, average='weighted')\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_params({\n",
    "            \"n_estimators\": config[\"n_estimators\"],\n",
    "            \"max_depth\": config[\"max_depth\"],\n",
    "            \"min_samples_split\": config[\"min_samples_split\"],\n",
    "            \"model_type\": \"RandomForestClassifier\",\n",
    "            \"dataset\": \"Bonsai Species\",\n",
    "            \"n_species\": len(bonsai_species),\n",
    "            \"description\": config[\"description\"]\n",
    "        })\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metrics({\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1,\n",
    "            \"test_samples\": len(y_test),\n",
    "            \"training_samples\": len(y_train)\n",
    "        })\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(\n",
    "            bonsai_classifier, \n",
    "            name=\"bonsai_classifier\",\n",
    "            signature=mlflow.models.infer_signature(X_train, y_train)\n",
    "        )\n",
    "        \n",
    "        # Log confusion matrix as artifact\n",
    "        cm = confusion_matrix(y_test, preds)\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        im = ax.imshow(cm, cmap='Blues')\n",
    "        ax.set_title('Confusion Matrix')\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "        plt.colorbar(im)\n",
    "        plt.xticks(np.arange(len(bonsai_species)), bonsai_species)\n",
    "        plt.yticks(np.arange(len(bonsai_species)), bonsai_species)\n",
    "        for i in range(len(bonsai_species)):\n",
    "            for j in range(len(bonsai_species)):\n",
    "                ax.text(j, i, cm[i, j], ha='center', va='center', color='black')\n",
    "        plt.tight_layout()\n",
    "        cm_path = f\"confusion_matrix_{config['name']}.png\"\n",
    "        plt.savefig(cm_path)\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(cm_path)\n",
    "        os.remove(cm_path)\n",
    "\n",
    "        # Log sample predictions as CSV artifact\n",
    "        sample_df = pd.DataFrame({\n",
    "            'actual': [bonsai_species[i] for i in y_test],\n",
    "            'predicted': [bonsai_species[i] for i in preds]\n",
    "        })\n",
    "        sample_path = f\"sample_predictions_{config['name']}.csv\"\n",
    "        sample_df.to_csv(sample_path, index=False)\n",
    "        mlflow.log_artifact(sample_path)\n",
    "        os.remove(sample_path)\n",
    "\n",
    "        # Save results for comparison\n",
    "        experiment_results.append({\n",
    "            \"name\": config[\"name\"],\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1,\n",
    "            \"description\": config[\"description\"]\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ {config['name']}: Accuracy={accuracy:.3f}, F1={f1:.3f}\")\n",
    "\n",
    "print(\"\\nüìä Experiment Summary:\")\n",
    "print(\"-\" * 60)\n",
    "for result in sorted(experiment_results, key=lambda x: x['accuracy'], reverse=True):\n",
    "    print(f\"üèÜ {result['name']}: {result['accuracy']:.3f} acc | {result['f1_score']:.3f} f1\")\n",
    "    print(f\"   üìù {result['description']}\")\n",
    "\n",
    "print(f\"\\nüåê Compare experiments in MLflow UI: http://localhost:5001\")\n",
    "print(\"üí° Use 'Compare' to view differences side by side!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8ddb4d-6c31-4034-b006-1066b0a46547",
   "metadata": {},
   "source": [
    "# üå± MLflow Model Registry and Experiment Comparison\n",
    "\n",
    "## Model Registry - Model Management\n",
    "The **MLflow Model Registry** is a centralized repository to manage the model lifecycle:\n",
    "\n",
    "### Main Features:\n",
    "- **üîÑ Versioning**: Each registered model receives a unique version\n",
    "- **üìã Stages**: None ‚Üí Staging ‚Üí Production ‚Üí Archived\n",
    "- **üè∑Ô∏è Tags and Annotations**: Custom metadata for organization\n",
    "- **üîç Lineage**: Tracking model origin (experiment/run)\n",
    "- **üöÄ Deploy**: Integration with deployment systems\n",
    "\n",
    "### Recommended Workflow:\n",
    "1. **Experiments**: Multiple runs to find the best model\n",
    "2. **Registration**: Register the best model in the registry\n",
    "3. **Staging**: Test model in staging environment\n",
    "4. **Production**: Promote after complete validation\n",
    "5. **Monitoring**: Track performance in production\n",
    "\n",
    "## Experiment Comparison\n",
    "Use comparison features for:\n",
    "- **üìä Side-by-side metrics**: Accuracy, F1-score, Precision, Recall\n",
    "- **‚öôÔ∏è Hyperparameters**: Compare different configurations\n",
    "- **üìà Visualizations**: Automatic performance charts\n",
    "- **üîÑ Reproducibility**: All details to reproduce results\n",
    "\n",
    "### MLflow Tip:\n",
    "Use `mlflow.autolog()` for automatic capture of metrics, parameters and artifacts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b85654-8662-4ca9-8950-63968ef900f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and register the best model based on experiments\n",
    "from mlflow.tracking import MlflowClient\n",
    "import mlflow.pyfunc\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# Find the best run based on accuracy metric\n",
    "experiment = mlflow.get_experiment_by_name(\"Bonsai-Species-Classification\")\n",
    "best_run = client.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    order_by=[\"metrics.accuracy DESC\"],\n",
    "    max_results=1\n",
    ")[0]\n",
    "\n",
    "print(f\"üèÜ Best model found:\")\n",
    "print(f\"   Run ID: {best_run.info.run_id}\")\n",
    "print(f\"   Accuracy: {best_run.data.metrics['accuracy']:.3f}\")\n",
    "print(f\"   F1-Score: {best_run.data.metrics['f1_score']:.3f}\")\n",
    "print(f\"   Configuration: {best_run.data.params}\")\n",
    "\n",
    "# Register the best model in the Model Registry\n",
    "model_name = \"Bonsai-Species-Classifier-Production\"\n",
    "model_uri = f\"runs:/{best_run.info.run_id}/bonsai_classifier\"  # Use correct artifact path\n",
    "\n",
    "try:\n",
    "    # Register new model version\n",
    "    model_version = mlflow.register_model(\n",
    "        model_uri=model_uri,\n",
    "        name=model_name\n",
    "    )\n",
    "    print(f\"\\n‚úÖ Model registered successfully!\")\n",
    "    print(f\"üì¶ Name: {model_name}\")\n",
    "    print(f\"üî¢ Version: {model_version.version}\")\n",
    "    print(f\"üåê MLflow UI: http://localhost:5000/#/models/{model_name}\")\n",
    "\n",
    "    # Add tags and annotations to the registered model\n",
    "    client.set_model_version_tag(\n",
    "        name=model_name,\n",
    "        version=model_version.version,\n",
    "        key=\"use_case\",\n",
    "        value=\"plant_ecommerce_classification\"\n",
    "    )\n",
    "    client.set_model_version_tag(\n",
    "        name=model_name,\n",
    "        version=model_version.version,\n",
    "        key=\"model_type\",\n",
    "        value=\"RandomForestClassifier\"\n",
    "    )\n",
    "    # Update description with detailed information\n",
    "    client.update_model_version(\n",
    "        name=model_name,\n",
    "        version=model_version.version,\n",
    "        description=f\"\"\"\n",
    "        üå≥ Bonsai Species Classifier for E-commerce\n",
    "\n",
    "        **Performance:**\n",
    "        - Accuracy: {best_run.data.metrics['accuracy']:.3f}\n",
    "        - Precision: {best_run.data.metrics['precision']:.3f}\n",
    "        - Recall: {best_run.data.metrics['recall']:.3f}\n",
    "        - F1-Score: {best_run.data.metrics['f1_score']:.3f}\n",
    "\n",
    "        **Configuration:**\n",
    "        - Estimators: {best_run.data.params['n_estimators']}\n",
    "        - Max Depth: {best_run.data.params['max_depth']}\n",
    "        - Min Samples Split: {best_run.data.params['min_samples_split']}\n",
    "\n",
    "        **Use Cases:**\n",
    "        - Automatic bonsai species identification\n",
    "        - Care recommendations based on species\n",
    "        - Purchase decision support for customers\n",
    "\n",
    "        **Classes:** Juniper (0), Ficus (1), Pine (2), Maple (3)\n",
    "        \"\"\"\n",
    "    )\n",
    "    print(f\"ÔøΩÔ∏è  Tags and description added to the model\")\n",
    "\n",
    "    # Transition model version to Staging (best practice)\n",
    "    client.transition_model_version_stage(\n",
    "        name=model_name,\n",
    "        version=model_version.version,\n",
    "        stage=\"Staging\"\n",
    "    )\n",
    "    print(f\"üö¶ Model version {model_version.version} transitioned to Staging.\")\n",
    "\n",
    "    # To promote to Production after validation:\n",
    "    # client.transition_model_version_stage(\n",
    "    #     name=model_name,\n",
    "    #     version=model_version.version,\n",
    "    #     stage=\"Production\"\n",
    "    # )\n",
    "    # print(f\"üöÄ Model version {model_version.version} promoted to Production.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error registering model: {e}\")\n",
    "    print(\"üí° Check if the MLflow server is running\")\n",
    "\n",
    "print(f\"\\nüìà Next steps:\")\n",
    "print(f\"1. Review model in MLflow UI\")\n",
    "print(f\"2. Test model in staging\")\n",
    "print(f\"3. Promote to production when approved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaf5cd5",
   "metadata": {},
   "source": [
    "# MLflow 3.3.1: Experiments, Artifact Logging, and Model Registry Best Practices\n",
    "\n",
    "**Experiments & Artifact Logging**\n",
    "- Track multiple experiments (runs) with parameters, metrics, and artifacts.\n",
    "- Log artifacts (models, plots, files) using `mlflow.log_artifact()` or `mlflow.<flavor>.log_model()`.\n",
    "- Artifacts help reproduce results and compare runs.\n",
    "\n",
    "**Model Registry Workflow**\n",
    "- Register your best model with `mlflow.register_model(model_uri, name)`.\n",
    "- Registry supports model versioning and lifecycle stages: `None`, `Staging`, `Production`, `Archived`.\n",
    "- Use MLflow UI or `MlflowClient` to transition model versions:\n",
    "  - `client.transition_model_version_stage(name, version, stage=\"Staging\")`\n",
    "  - `client.transition_model_version_stage(name, version, stage=\"Production\")`\n",
    "\n",
    "**Identifying Staging/Production Versions**\n",
    "- Assign tags and descriptions to model versions for clarity.\n",
    "- Promote/demote versions between stages using UI or API.\n",
    "- Only one version should be in `Production` at a time for a given model.\n",
    "\n",
    "**Best Practices**\n",
    "- Always log relevant artifacts for each run.\n",
    "- Use clear naming and tagging for models and versions.\n",
    "- Automate stage transitions as part of your CI/CD pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8398b505",
   "metadata": {},
   "source": [
    "## üî¨ Test Model in Staging\n",
    "\n",
    "Before promoting a model to production, it's essential to validate its performance in a controlled staging environment. Here, we load the model from the MLflow registry (in the 'Staging' stage) and run predictions on new data. This step ensures the model meets quality standards and behaves as expected before serving real users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aafce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model in staging\n",
    "from mlflow.pyfunc import load_model\n",
    "\n",
    "# Get the latest model version in staging\n",
    "model_name = \"Bonsai-Species-Classifier-Production\"\n",
    "from mlflow.tracking import MlflowClient\n",
    "client = MlflowClient()\n",
    "staging_versions = [v for v in client.get_latest_versions(model_name, stages=[\"Staging\"])]\n",
    "if staging_versions:\n",
    "    staging_model_uri = f\"models:/{model_name}/Staging\"\n",
    "    print(f\"Loading model from: {staging_model_uri}\")\n",
    "    model = load_model(staging_model_uri)\n",
    "    # Example prediction (using first test sample)\n",
    "    sample = X_test[0].reshape(1, -1)\n",
    "    pred = model.predict(sample)\n",
    "    print(f\"Predicted class for first test sample: {bonsai_species[pred[0]]}\")\n",
    "else:\n",
    "    print(\"No model in Staging stage found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc62b685",
   "metadata": {},
   "source": [
    "## üöÄ Promote Model to Production\n",
    "\n",
    "Once the model passes all tests in staging, it's ready to be promoted to production. This step updates the model's stage in the MLflow registry, making it available for real-world predictions. Production models should be monitored for performance and retrained as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0f89e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promote model to production when approved\n",
    "model_name = \"Bonsai-Species-Classifier-Production\"\n",
    "from mlflow.tracking import MlflowClient\n",
    "client = MlflowClient()\n",
    "staging_versions = [v for v in client.get_latest_versions(model_name, stages=[\"Staging\"])]\n",
    "if staging_versions:\n",
    "    version = staging_versions[0].version\n",
    "    client.transition_model_version_stage(\n",
    "        name=model_name,\n",
    "        version=version,\n",
    "        stage=\"Production\"\n",
    "    )\n",
    "    print(f\"üöÄ Model version {version} promoted to Production.\")\n",
    "else:\n",
    "    print(\"No model in Staging stage to promote.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c143748",
   "metadata": {},
   "source": [
    "# üéì What Did We Learn?\n",
    "\n",
    "In this notebook, you:\n",
    "- Built and tracked a bonsai species classifier using MLflow\n",
    "- Logged parameters, metrics, and artifacts for reproducibility\n",
    "- Registered your model and managed its lifecycle (Staging ‚Üí Production)\n",
    "- Validated model performance before production deployment\n",
    "\n",
    "## üõ†Ô∏è Next Steps\n",
    "- **Monitor** your production model for drift and performance\n",
    "- **Automate** retraining and deployment with CI/CD pipelines\n",
    "- **Serve** your model via an API for real-time predictions\n",
    "- **Integrate** with business systems for end-to-end MLOps\n",
    "\n",
    "Explore MLflow's advanced features and try deploying your own models in a real project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e79802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Bonsai Species Classification API\n",
    "import requests\n",
    "\n",
    "# Example input features: [leaf_length_cm, leaf_width_cm, branch_thickness_mm, height_cm]\n",
    "features = [5.2, 3.1, 2.0, 25.0]\n",
    "\n",
    "url = \"http://api:8080/predict\"\n",
    "payload = {\"features\": features}\n",
    "\n",
    "try:\n",
    "    response = requests.post(url, json=payload)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    print(\"Prediction result:\", result)\n",
    "except Exception as e:\n",
    "    print(f\"API call failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1147852a-7b3b-412e-bd1b-e19f802e29ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
